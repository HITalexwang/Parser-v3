#**************************************************************
[DEFAULT]
LANGUAGE = Chinese
LC = zh
TREEBANK = sdp
TB = sdp
network_class = GraphParserNetwork
save_dir = ${save_metadir}/${network_class}

test_conllus = examples/data/trial.test.conllu
train_conllus = examples/data/trial.train.conllu
dev_conllus = examples/data/trial.valid.conllu

aux_conllus =
cpu_num = 16
intra_threads = -1
inter_threads = -1

#***************************************************************
# Network
[Config]

[BaseNetwork]
n_passes = 0
max_steps = 100
max_steps_without_improvement = 50
print_every = 10
save_model_after_improvement = True
save_model_after_training = False
parse_devset = False
switch_optimizers = False
use_bert_adam = True
warm_up_steps = 10

# neural
l2_reg = 0.
output_keep_prob = .5
conv_keep_prob = .5
recur_keep_prob = .67
recur_include_prob = 1.
#hidden_keep_prob = .67
n_layers = 3
first_layer_conv_width = 0
conv_width = 0
output_size = 100
recur_size = 200
output_func = identity
bidirectional = True
recur_cell = LSTM
recur_func = tanh
cifg = False
# TODO try highway concatenation instead of addition
highway = False
highway_func = tanh
bilin = False
share_layer = False

[ElmoNetwork]
input_vocab_classes = FormSubtokenVocab
output_vocab_classes = FormTokenVocab
throughput_vocab_classes =
input_network_classes = None
#neural
recur_size = 500
n_layers = 2
n_samples = 1000

[TaggerNetwork]
input_vocab_classes = FormMultivocab
output_vocab_classes = UPOSTokenVocab:XPOSTokenVocab:UFeatsFeatureVocab
throughput_vocab_classes = LemmaTokenVocab:DepheadIndexVocab:DeprelTokenVocab
input_network_classes = None
#neural
n_layers = 2
recur_keep_prob = .5
recur_size = 200

[ParserNetwork]
input_vocab_classes = FormMultivocab:UPOSTokenVocab:XPOSTokenVocab:UFeatsFeatureVocab:LemmaTokenVocab
output_vocab_classes = DepheadIndexVocab:DeprelTokenVocab
throughput_vocab_classes =
input_network_classes = None
sum_pos = True
recur_size = 400

[GraphParserNetwork]
;input_vocab_classes = FormMultivocab:UPOSTokenVocab
input_vocab_classes = FormBERTVocab:UPOSTokenVocab
output_vocab_classes = SemheadGraphIndexVocab:SemrelGraphTokenVocab
throughput_vocab_classes =
input_network_classes = None
sum_pos = False
recur_size = 600
n_layers = 3
conv_keep_prob = .55
recur_keep_prob = .75

[GraphMTLNetwork]
input_vocab_classes = FormMultivocab:UPOSTokenVocab
output_vocab_classes = SemheadGraphIndexVocab:SemrelGraphTokenVocab
throughput_vocab_classes =
input_network_classes = None
sum_pos = False
recur_size = 600
n_layers = 3
conv_keep_prob = .55
recur_keep_prob = .75
aux_hidden_size = 400
share_head_mlp = True

[GraphOutputs]
decoder = sem16

#**************************************************************
# CoNLLU fields
[CoNLLUVocab]

[FormVocab]
[LemmaVocab]
[UPOSVocab]
[XPOSVocab]
[UFeatsVocab]
[DepheadVocab]
[DeprelVocab]
[SemrelVocab]
[SemheadVocab]

#***************************************************************
# Datasets
[CoNLLUDataset]
max_buckets = 1
batch_size = 4096

[CoNLLUTrainset]
max_buckets = 1
batch_size = 512

[CoNLLUAuxset]
max_buckets = 1
batch_size = 512

[CoNLLUDevset]
max_buckets = 1
batch_size = 512

[CoNLLUTestset]

#**************************************************************
# Vocabulary types
[BaseVocab]

#===============================================================
# Numeric vocabs
[IndexVocab]
#neural
hidden_size = 600
hidden_keep_prob = .5
add_linear = True
n_layers = 1
hidden_func = leaky_relu
diagonal = False
linearize = False
distance = False

[IDIndexVocab]

[DepheadIndexVocab]

[SemheadGraphIndexVocab]
hidden_size = 600
hidden_keep_prob = .75
n_layers = 1
fp_cost = -1
fn_cost = -1

#===============================================================
# String Vocabs
[SetVocab]
cased = None
special_token_case = None
special_token_html = None
max_embed_count = 0
vocab_loadname =

[PretrainedVocab]
cased = False
special_token_case = upper
special_token_html = True
max_embed_count = 0
pretrained_file = None
name = None
vocab_loadname = ${save_dir}/Embedding/embedding.pkl
save_as_pickle = True
# neural
linear_size = 125
embed_keep_prob = .80

[FormPretrainedVocab]
pretrained_file = examples/data/trial.embedding
name = trans

#===============================================================
# Token vocabs
[CountVocab]
cased = None
min_occur_count = None

[TokenVocab]
cased = True
special_token_case = upper
special_token_html = True
min_occur_count = 1
# neural
embed_size = 100
embed_keep_prob = .80
drop_func = unkout
hidden_size = 400
hidden_keep_prob = .67
n_layers = 1
add_linear = True
hidden_func = leaky_relu
diagonal = False

[FormTokenVocab]
cased = True
min_occur_count = 7
#embed_size = ${SubtokenVocab:output_size}
embed_size = 100

[LemmaTokenVocab]
cased = True
min_occur_count = 7
embed_size = 100
embed_keep_prob = .80

[UPOSTokenVocab]
special_token_html = False
embed_size = 100
embed_keep_prob = .80

[XPOSTokenVocab]
special_token_html = False
embed_size = 100
embed_keep_prob = .80

[DeprelTokenVocab]
special_token_case = lower
special_token_html = False
factorized = True
# neural
hidden_size = 200
diagonal = False
add_linear = True
loss_interpolation = .5

[SemrelGraphTokenVocab]
hidden_size = 600
hidden_keep_prob = .67
n_layers = 1
special_token_case = lower
special_token_html = False
factorized = True
# neural
add_linear = True
loss_interpolation = .4

#===============================================================
# Subtoken vocabs
[SubtokenVocab]
cased = True
special_token_case = upper
special_token_html = True
min_occur_count = 1
max_buckets = 2
token_vocab_loadname =
# neural
embed_size = 100
embed_keep_prob = .67
conv_keep_prob = .67
recur_keep_prob = .67
recur_include_prob = 1.
output_keep_prob = .67
n_layers = 1
first_layer_conv_width = 0
conv_width = 0
recur_size = 400
bidirectional = False
recur_cell = LSTM
recur_func = tanh
output_func = identity
cifg = False
highway = False
highway_func = identity
bilin = False
squeeze_type = linear_attention
output_size = 100

[FormSubtokenVocab]
min_occur_count = 7

[LemmaSubtokenVocab]
min_occur_count = 7

#===============================================================
# BERT vocabs
[BERTVocab]
cased = True
special_token_case = upper
special_token_html = True
min_occur_count = 1
;max_buckets = 1
token_vocab_loadname =
;bert_hub_path = bert/hub_bert_chinese_L-12_H-768_A-12
vocab_file = bert/chinese_L-12_H-768_A-12/vocab.txt
config_file = bert/chinese_L-12_H-768_A-12/bert_config.json
pretrained_ckpt = bert/chinese_L-12_H-768_A-12/bert_model.ckpt
do_lower_case = True
trainable = True
embed_keep_prob = 1.
drop_func = unkout

[FormBERTVocab]
min_occur_count = 1

[LemmaBERTVocab]
min_occur_count = 7

#===============================================================
# Feature vocabs
[FeatureVocab]
vocab_loadname =
pad_str =
separator =
keyed = False
cased = True
min_occur_count = 0
max_embed_count = 0
# neural
hidden_keep_prob = .5
n_layers = 1
hidden_size = 100
hidden_func = leaky_relu
embed_keep_prob = .67
drop_func = unkout
embed_size = 50
diagonal = False
add_linear = True

[LemmaFeatureVocab]
separator = +
min_occur_count = 2

[XPOSFeatureVocab]
pad_str = -

[UFeatsFeatureVocab]
separator = |
keyed = True

#===============================================================
# Multivocabs
# TODO rework multivocabs
[Multivocab]
use_token_vocab = True
use_subtoken_vocab = False
use_bert_vocab = False
use_pretrained_vocab = True
pretrained_files = None
names = None
# neural
combine_func = concat
embed_keep_prob = .80
drop_func = unkout

[FormMultivocab]
use_token_vocab = True
use_pretrained_vocab = True
use_subtoken_vocab = True
use_bert_vocab = True
use_elmo_vocab = False

#***************************************************************
# Optimization
[Optimizer]
learning_rate = 1e-3
bert_learning_rate = 2e-5
decay_rate = 0
clip = 1.
mu = 0
nu = .95
epsilon = 1e-12
gamma = 0

[AMSGradOptimizer]
